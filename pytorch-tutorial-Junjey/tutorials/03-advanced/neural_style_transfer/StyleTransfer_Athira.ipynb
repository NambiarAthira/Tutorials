{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion() \n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "def load_image(image_path, transform=transform, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)\n",
    "\n",
    "\n",
    "# img_path='/media/athira/CVlab_IITM/CV lab documents/Works/DeepLearning course/extraa/pytorch from web/pytorch-tutorial-Junjey/tutorials/03-advanced'\n",
    "# img_name=os.listdir(img_path)\n",
    "# image=Image.open(os.path.join(img_path,img_name[0]))\n",
    "# image = ToTensor()(image).unsqueeze(0) # unsqueeze to add artificial first dimension\n",
    "# image=image.to(device)\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "net = VGGNet().to(device)\n",
    "# print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/athira/CVlab_IITM/CV lab documents/Works/DeepLearning course/extraa/pytorch from web/pytorch-tutorial-Junjey/tutorials/03-advanced/neural_style_transfer/png/content.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path='/media/athira/CVlab_IITM/CV lab documents/Works/DeepLearning course/extraa/pytorch from web/pytorch-tutorial-Junjey/tutorials/03-advanced/neural_style_transfer'\n",
    "content=os.path.join(img_path,'png/content.png')\n",
    "style=os.path.join(img_path,'png/style.png')\n",
    "max_size=400\n",
    "total_step=2000\n",
    "log_step=10\n",
    "sample_step=500\n",
    "style_weight=100\n",
    "\n",
    "content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2552\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2553\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dd0a5a7dbfa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-352e2192567d>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_path, transform, max_size, shape)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"\"\"Load an image and convert it to a torch tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2552\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2554\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2555\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "content = load_image(content, transform, max_size=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We use the same normalization statistics here.\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "#                              std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image same size as the content image\n",
    "    content = load_image(content, transform, max_size=config.max_size)\n",
    "    style = load_image(style, transform, shape=[content.size(0), content.size(1)])\n",
    "\n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "main()\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--content', type=str, default='./png/content.png')\n",
    "#     parser.add_argument('--style', type=str, default='./png/style.png')\n",
    "#     parser.add_argument('--max_size', type=int, default=400)\n",
    "#     parser.add_argument('--total_step', type=int, default=2000)\n",
    "#     parser.add_argument('--log_step', type=int, default=10)\n",
    "#     parser.add_argument('--sample_step', type=int, default=500)\n",
    "#     parser.add_argument('--style_weight', type=float, default=100)\n",
    "#     parser.add_argument('--lr', type=float, default=0.003)\n",
    "#     config = parser.parse_args()\n",
    "#     print(config)\n",
    "#     main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
